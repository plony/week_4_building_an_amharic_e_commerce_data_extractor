{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plony/week_4_building_an_amharic_e_commerce_data_extractor/blob/main/Taks_6_FinTech_Vendor_Scorecard_for_Micro_Lending.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 4: Model Comparison & Selection**"
      ],
      "metadata": {
        "id": "RYR6iqMjUh3j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evtH6U9-lkkk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate seqeval -q\n",
        "!pip install optimum -q # Optional: For ONNX export or quantization later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n65Ks2n6rcdv"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95ddQUr1rN5X"
      },
      "outputs": [],
      "source": [
        "print(\"\\n2. Importing libraries and loading model components...\")\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "from datasets import load_dataset, Dataset, Features, Value, ClassLabel, Sequence\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the model checkpoint\n",
        "# Choose one of the following models by uncommenting it:\n",
        "print(\"\\n2. Importing libraries and loading model components...\")\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "from datasets import load_dataset, Dataset, Features, Value, ClassLabel, Sequence\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the model checkpoint\n",
        "# Choose one of the following models by uncommenting it:\n",
        "# MODEL_CHECKPOINT = \"xlm-roberta-base\"        # Strong general-purpose multilingual model\n",
        "# MODEL_CHECKPOINT = \"attributio/bert-tiny-amharic\" # Smaller, faster, Amharic-specific\n",
        "MODEL_CHECKPOINT = \"bert-base-multilingual-cased\"  # Good multilingual model for African languages\n",
        "\n",
        "# Load tokenizer\n",
        "print(f\"Loading tokenizer from: {MODEL_CHECKPOINT}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Define your labels - these MUST match exactly with your CoNLL labels\n",
        "# IMPORTANT: Ensure this list contains all unique B-I-O tags from your labeled_telegram_product_price_location.txt\n",
        "label_list = [\n",
        "    \"O\",\n",
        "    \"B-PRODUCT\", # Changed P to uppercase\n",
        "    \"I-PRODUCT\", # Changed P to uppercase\n",
        "    \"B-PRICE\",\n",
        "    \"I-PRICE\",\n",
        "    \"B-LOC\",\n",
        "    \"I-LOC\"\n",
        "]\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "print(f\"Defined labels: {label_list}\")\n",
        "print(f\"id2label mapping: {id2label}\")\n",
        "print(f\"label2id mapping: {label2id}\")\n",
        "print(f\"Loading tokenizer from: {MODEL_CHECKPOINT}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Define your labels - these MUST match exactly with your CoNLL labels\n",
        "# IMPORTANT: Ensure this list contains all unique B-I-O tags from your labeled_telegram_product_price_location.txt\n",
        "label_list = [\n",
        "    \"O\",\n",
        "    \"B-PRODUCT\", # Changed P to uppercase\n",
        "    \"I-PRODUCT\", # Changed P to uppercase\n",
        "    \"B-PRICE\",\n",
        "    \"I-PRICE\",\n",
        "    \"B-LOC\",\n",
        "    \"I-LOC\"\n",
        "]\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "print(f\"Defined labels: {label_list}\")\n",
        "print(f\"id2label mapping: {id2label}\")\n",
        "print(f\"label2id mapping: {label2id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKNiGuGLrr2m"
      },
      "outputs": [],
      "source": [
        "print(\"\\n3. Loading the labeled dataset...\")\n",
        "\n",
        "\n",
        "# Run this cell, a file uploader will appear.  .txt file.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "uploaded_file_name = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {uploaded_file_name}\")\n",
        "file_name = uploaded_file_name # Use the uploaded file name\n",
        "\n",
        "\n",
        "\n",
        "# # from google.colab import drive\n",
        "# # drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "# # print(f\"Looking for file at: {file_path}\")\n",
        "# # file_name = file_path # Use the full path as the file_name\n",
        "\n",
        "\n",
        "# Function to parse CoNLL formatted file\n",
        "def parse_conll_file(file_path):\n",
        "    \"\"\"Parses a CoNLL formatted file into a list of (words, tags) tuples.\"\"\"\n",
        "    texts = []\n",
        "    tags = []\n",
        "    current_words = []\n",
        "    current_tags = []\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line: # If line is not empty\n",
        "                parts = line.split()\n",
        "                if len(parts) == 2:\n",
        "                    word, tag = parts[0], parts[1]\n",
        "                    current_words.append(word)\n",
        "                    current_tags.append(tag)\n",
        "                else:\n",
        "                    # Handle malformed lines: if a line is not empty but doesn't have 2 parts, it's malformed\n",
        "                    print(f\"Warning: Skipping malformed line (expected 2 parts, got {len(parts)}): '{line}'\")\n",
        "            else: # Empty line indicates end of a sentence/message\n",
        "                if current_words: # Only add if there are words in the current sentence\n",
        "                    texts.append(current_words)\n",
        "                    tags.append(current_tags)\n",
        "                    current_words = []\n",
        "                    current_tags = []\n",
        "    # Add any remaining sentence at the end of the file (important if file doesn't end with blank line)\n",
        "    if current_words:\n",
        "        texts.append(current_words)\n",
        "        tags.append(current_tags)\n",
        "\n",
        "    return texts, tags\n",
        "\n",
        "\n",
        "raw_texts, raw_tags = parse_conll_file(file_name)\n",
        "print(f\"Successfully parsed {len(raw_texts)} sentences from the CoNLL file.\")\n",
        "\n",
        "# Check for any tags in data that are not in label_list\n",
        "all_unique_tags_in_data = set(tag for sublist in raw_tags for tag in sublist)\n",
        "missing_labels_in_config = all_unique_tags_in_data - set(label_list)\n",
        "if missing_labels_in_config:\n",
        "    print(f\"\\nWARNING: Found tags in your data not present in 'label_list': {missing_labels_in_config}\")\n",
        "    print(\"Please update 'label_list' in Cell 2 to include these tags and rerun all cells from the beginning.\")\n",
        "    # Consider adding `raise ValueError(\"Missing labels in config\")`\n",
        "    # if this critical issue occurs.\n",
        "\n",
        "\n",
        "# Convert raw_tags (string labels) to numerical IDs\n",
        "numeric_tags = []\n",
        "for i, sentence_tags in enumerate(raw_tags):\n",
        "    current_numeric_tags = []\n",
        "    for tag in sentence_tags:\n",
        "        if tag in label2id:\n",
        "            current_numeric_tags.append(label2id[tag])\n",
        "        else:\n",
        "            # This case should ideally be caught by the warning above.\n",
        "            print(f\"Error: Tag '{tag}' not found in label2id for sentence {i}. Assigning 'O'.\")\n",
        "            current_numeric_tags.append(label2id[\"O\"])\n",
        "    numeric_tags.append(current_numeric_tags)\n",
        "\n",
        "\n",
        "# Create a Hugging Face Dataset\n",
        "features = Features({\n",
        "    'id': Value('string'),\n",
        "    'tokens': Sequence(Value('string')),\n",
        "    'ner_tags': Sequence(ClassLabel(names=label_list))\n",
        "})\n",
        "\n",
        "data_dict_list = []\n",
        "for i, (tokens, tags) in enumerate(zip(raw_texts, numeric_tags)):\n",
        "    # Ensure tokens and tags have the same length\n",
        "    if len(tokens) != len(tags):\n",
        "        print(f\"Warning: Token-tag length mismatch in sentence {i}. Skipping this sentence.\")\n",
        "        print(f\"Tokens: {tokens}\")\n",
        "        print(f\"Tags: {[id2label[t] for t in tags]}\") # Convert numerical tags back to string for printing\n",
        "        continue # Skip this malformed sentence\n",
        "    data_dict_list.append({\n",
        "        'id': str(i),\n",
        "        'tokens': tokens,\n",
        "        'ner_tags': tags\n",
        "    })\n",
        "\n",
        "dataset = Dataset.from_list(data_dict_list, features=features)\n",
        "print(f\"Dataset loaded with {len(dataset)} examples.\")\n",
        "print(\"Example from dataset (first entry):\")\n",
        "print(dataset[0])\n",
        "\n",
        "# Split into training and validation sets\n",
        "# small validation set (e.g., 10-20% of data). Adjust test_size as needed.\n",
        "train_test_split = dataset.train_test_split(test_size=0.2, seed=42) # Added seed for reproducibility\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
        "print(f\"Eval dataset size: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqhtST3cr3dX"
      },
      "outputs": [],
      "source": [
        "print(\"\\n4. Tokenizing data and aligning labels...\")\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    # This function expects examples['tokens'] to be a list of lists of words (sentences)\n",
        "    # and examples['ner_tags'] to be a list of lists of numerical tag IDs.\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True, # Truncate long sequences to model's max input length\n",
        "        is_split_into_words=True # Tells the tokenizer that inputs are already pre-split into words\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]): # Iterate through each sentence's original labels\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i) # Get word IDs for the current tokenized sentence\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens (like CLS, SEP, PAD) have a word_idx of None.\n",
        "            # We set their label to -100 so they are ignored in loss computation.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # If this is the first token of a new word, assign its original label.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # If it's a subsequent subword token of the same word:\n",
        "            else:\n",
        "                # Get the original string label for the word\n",
        "                original_label_str = id2label[label[word_idx]]\n",
        "                # If the original label was a 'B-' tag, change it to 'I-'.\n",
        "                # Otherwise, keep it as 'I-' or 'O'. This ensures all subwords of an entity\n",
        "                # are labeled as 'I-' (or 'O' if the word was 'O').\n",
        "                if original_label_str.startswith(\"B-\"):\n",
        "                    label_ids.append(label2id[f\"I-{original_label_str[2:]}\"])\n",
        "                else:\n",
        "                    label_ids.append(label[word_idx]) # For I- and O tags, keep them as is\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the tokenization and alignment to both training and evaluation datasets\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "print(\"\\nExample of tokenized and aligned data (first entry from training set):\")\n",
        "first_example_tokenized = tokenized_train_dataset[0]\n",
        "print(\"Original Tokens (first sentence in training set):\", train_dataset[0][\"tokens\"])\n",
        "print(\"Original Labels:\", [id2label[l] for l in train_dataset[0][\"ner_tags\"]])\n",
        "print(\"Subword Tokens (after tokenization):\", tokenizer.convert_ids_to_tokens(first_example_tokenized[\"input_ids\"]))\n",
        "print(\"Aligned Numerical Labels:\", first_example_tokenized[\"labels\"])\n",
        "print(\"Decoded Aligned Labels:\", [id2label[l] if l != -100 else \"IGNORE\" for l in first_example_tokenized[\"labels\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL4-fEOFtCcn"
      },
      "outputs": [],
      "source": [
        "print(\"\\n4. Tokenizing data and aligning labels...\")\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    # This function expects examples['tokens'] to be a list of lists of words (sentences)\n",
        "    # and examples['ner_tags'] to be a list of lists of numerical tag IDs.\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True, # Truncate long sequences to model's max input length\n",
        "        is_split_into_words=True # Tells the tokenizer that inputs are already pre-split into words\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]): # Iterate through each sentence's original labels\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i) # Get word IDs for the current tokenized sentence\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens (like CLS, SEP, PAD) have a word_idx of None.\n",
        "            # We set their label to -100 so they are ignored in loss computation.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # If this is the first token of a new word, assign its original label.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # If it's a subsequent subword token of the same word:\n",
        "            else:\n",
        "                # Get the original string label for the word\n",
        "                original_label_str = id2label[label[word_idx]]\n",
        "                # If the original label was a 'B-' tag, change it to 'I-'.\n",
        "                # Otherwise, keep it as 'I-' or 'O'. This ensures all subwords of an entity\n",
        "                # are labeled as 'I-' (or 'O' if the word was 'O').\n",
        "                if original_label_str.startswith(\"B-\"):\n",
        "                    label_ids.append(label2id[f\"I-{original_label_str[2:]}\"])\n",
        "                else:\n",
        "                    label_ids.append(label[word_idx]) # For I- and O tags, keep them as is\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the tokenization and alignment to both training and evaluation datasets\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "print(\"\\nExample of tokenized and aligned data (first entry from training set):\")\n",
        "first_example_tokenized = tokenized_train_dataset[0]\n",
        "print(\"Original Tokens (first sentence in training set):\", train_dataset[0][\"tokens\"])\n",
        "print(\"Original Labels:\", [id2label[l] for l in train_dataset[0][\"ner_tags\"]])\n",
        "print(\"Subword Tokens (after tokenization):\", tokenizer.convert_ids_to_tokens(first_example_tokenized[\"input_ids\"]))\n",
        "print(\"Aligned Numerical Labels:\", first_example_tokenized[\"labels\"])\n",
        "print(\"Decoded Aligned Labels:\", [id2label[l] if l != -100 else \"IGNORE\" for l in first_example_tokenized[\"labels\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-2nMzNptJF2"
      },
      "outputs": [],
      "source": [
        "#Set up Training Arguments and Model\n",
        "print(\"\\n5. Setting up training arguments and model...\")\n",
        "\n",
        "# Initialize the Data Collator for Token Classification\n",
        "# This handles padding of sequences to the longest sequence in each batch,\n",
        "# and also stacks inputs into tensors.\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "# Load the model for token classification\n",
        "# This will add a classification head on top of the pre-trained model.\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=len(label_list), # Number of unique NER labels\n",
        "    id2label=id2label,         # Map numerical IDs back to string labels for output\n",
        "    label2id=label2id          # Map string labels to numerical IDs for internal use\n",
        ")\n",
        "\n",
        "# Verify if model's label configurations are correctly set\n",
        "print(f\"Model num_labels: {model.config.num_labels}\")\n",
        "print(f\"Model id2label: {model.config.id2label}\")\n",
        "print(f\"Model label2id: {model.config.label2id}\")\n",
        "\n",
        "# Define training arguments\n",
        "# These parameters significantly impact training time and model performance.\n",
        "# Adjust `num_train_epochs` and `per_device_train_batch_size` based on your dataset size\n",
        "# and available GPU memory.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",      # Evaluate at the end of each epoch (updated argument name)# Directory to save model checkpoints and training logs\n",
        "     learning_rate=2e-5,                       # Learning rate for the optimizer (typical for fine-tuning)\n",
        "    per_device_train_batch_size=16,           # Batch size per GPU/CPU during training\n",
        "    per_device_eval_batch_size=16,            # Batch size per GPU/CPU during evaluation\n",
        "    num_train_epochs=5,                       # Number of full passes over the training data\n",
        "    weight_decay=0.01,                        # L2 regularization to prevent overfitting\n",
        "    logging_dir=\"./logs\",                     # Directory for TensorBoard logs\n",
        "    logging_steps=100,                        # How often to log training information\n",
        "    save_strategy=\"epoch\",                    # Save a model checkpoint at the end of each epoch\n",
        "    save_total_limit=2,                       # Keep only the last 2 best checkpoints to save disk space\n",
        "    report_to=\"none\",                         # Disable integrations like Weights & Biases for simplicity\n",
        "    fp16=True,                                # Enable mixed precision training (float16) for faster GPU training\n",
        "    push_to_hub=False,                        # Do not push the model to the Hugging Face Hub automatically\n",
        "    load_best_model_at_end=True,              # Load the model with the best evaluation metric at the end of training\n",
        "    metric_for_best_model=\"overall_f1\",       # The metric to monitor for selecting the best model\n",
        "    greater_is_better=True,                   # For F1-score, a higher value is better\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhOmkx4JuhMQ"
      },
      "outputs": [],
      "source": [
        "#Initialize and Run the Hugging Face Trainer\n",
        "print(\"\\n6. Initializing and running the Hugging Face Trainer...\")\n",
        "\n",
        "# Define the compute_metrics function for NER evaluation using seqeval\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    # Convert prediction logits to predicted label IDs\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Convert numerical labels and predictions back to string labels for seqeval\n",
        "    # Also, remove ignored index (-100)\n",
        "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # --- Important for seqeval ---\n",
        "    # Ensure lengths of prediction and true label lists are identical for each sample.\n",
        "    # This addresses potential issues where `word_ids` or `prediction` might cause slight mismatches.\n",
        "    cleaned_true_predictions = []\n",
        "    cleaned_true_labels = []\n",
        "    for pred_list, label_list_i in zip(true_predictions, true_labels):\n",
        "        if len(pred_list) == len(label_list_i):\n",
        "            cleaned_true_predictions.append(pred_list)\n",
        "            cleaned_true_labels.append(label_list_i)\n",
        "        else:\n",
        "            # This should ideally not happen if tokenization and alignment are robust.\n",
        "            # Print a warning if a mismatch occurs, indicating a potential data or alignment issue.\n",
        "            print(f\"Warning: Skipping a sample in metrics calculation due to length mismatch: pred={len(pred_list)}, label={len(label_list_i)}\")\n",
        "\n",
        "    if not cleaned_true_labels: # Handle case where all samples are skipped or no valid labels\n",
        "        return {\"overall_precision\": 0.0, \"overall_recall\": 0.0, \"overall_f1\": 0.0, \"overall_accuracy\": 0.0}\n",
        "\n",
        "    # Generate the classification report from seqeval\n",
        "    report = classification_report(cleaned_true_labels, cleaned_true_predictions, output_dict=True)\n",
        "\n",
        "    # Extract overall metrics, typically using 'micro avg' for overall performance in NER\n",
        "    overall_f1 = report['micro avg']['f1-score'] if 'micro avg' in report else f1_score(cleaned_true_labels, cleaned_true_predictions, average='micro')\n",
        "    overall_precision = report['micro avg']['precision'] if 'micro avg' in report else precision_score(cleaned_true_labels, cleaned_true_predictions, average='micro')\n",
        "    overall_recall = report['micro avg']['recall'] if 'micro avg' in report else recall_score(cleaned_true_labels, cleaned_true_predictions, average='micro')\n",
        "    overall_accuracy = accuracy_score(cleaned_true_labels, cleaned_true_predictions)\n",
        "\n",
        "    metrics = {\n",
        "        \"overall_precision\": overall_precision,\n",
        "        \"overall_recall\": overall_recall,\n",
        "        \"overall_f1\": overall_f1,\n",
        "        \"overall_accuracy\": overall_accuracy,\n",
        "    }\n",
        "\n",
        "    # Add per-entity F1 scores if they exist in the report (excluding 'O' tag and 'micro avg')\n",
        "    for entity_type in label_list:\n",
        "        # Check if the entity type is present in the report (i.e., it appeared in the eval set)\n",
        "        if entity_type != 'O' and entity_type in report:\n",
        "            metrics[f\"{entity_type}_f1\"] = report[entity_type]['f1-score']\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset, # The model will be evaluated on this dataset\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics, # Function to compute evaluation metrics\n",
        ")\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "# This will start the training loop. Progress bars and metrics will be displayed.\n",
        "trainer.train()\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb1_uyRXu3fl"
      },
      "outputs": [],
      "source": [
        "#Evaluate the Fine-tuned Model (Final Evaluation)\n",
        "print(\"\\n7. Evaluating the fine-tuned model on the validation set (final check)...\")\n",
        "# This will run a final evaluation on the `eval_dataset` using the best model loaded at the end of training.\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Final Evaluation Results:\", eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhIjZwY2xB6m"
      },
      "outputs": [],
      "source": [
        "print(\"\\n8. Saving the fine-tuned model and tokenizer...\")\n",
        "\n",
        "model_save_path = \"./fine_tuned_amharic_ner_model_v1\" # Local path in Colab (temporary, deleted when session ends)\n",
        "\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Save the model's weights, configuration, and vocabulary\n",
        "trainer.save_model(model_save_path) # Saves the model's weights and configuration\n",
        "tokenizer.save_pretrained(model_save_path) # Saves the tokenizer files (vocab, merges, etc.)\n",
        "\n",
        "print(f\"Model and tokenizer saved successfully to: {model_save_path}\")\n",
        "\n",
        "\n",
        "# This zips the model directory and initiates a browser download.\n",
        "try:\n",
        "    if not \"MyDrive\" in model_save_path: # Only attempt download if not saved to Drive\n",
        "        print(\"\\nAttempting to zip and download the model (if saved locally)...\")\n",
        "        !zip -r /content/fine_tuned_amharic_ner_model_v1.zip {model_save_path}\n",
        "        from google.colab import files\n",
        "        files.download('/content/fine_tuned_amharic_ner_model_v1.zip')\n",
        "        print(\"Model zip file download initiated.\")\n",
        "    else:\n",
        "        print(\"Model saved to Google Drive, no need to download from Colab local storage.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not zip or download model (an error occurred or it was saved to Drive): {e}\")\n",
        "\n",
        "print(\"\\n--- Fine-tuning process complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S3RjwhDmUJtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 5: Model Interpretability**"
      ],
      "metadata": {
        "id": "OQYZReMFQU6x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KInRyW7IJp-"
      },
      "outputs": [],
      "source": [
        "# New Cell: Install Interpretability Libraries\n",
        "print(\"Installing interpretability libraries (SHAP, LIME)...\")\n",
        "!pip install shap lime -q\n",
        "print(\"Interpretability libraries installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2s3B6ETSBa6"
      },
      "outputs": [],
      "source": [
        "# Load Fine-tuned mBERT Model\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "model_load_path = \"/content/fine_tuned_amharic_ner_model_v1\"\n",
        "\n",
        "if not os.path.exists(model_load_path):\n",
        "    print(f\"Error: Model not found at {model_load_path}. Please ensure the path is correct and the model was saved.\")\n",
        "else:\n",
        "    print(f\"Loading model from: {model_load_path}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_load_path)\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_load_path)\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # Retrieve label mappings from the loaded model's config\n",
        "    id2label = model.config.id2label\n",
        "    label2id = model.config.label2id\n",
        "    label_list = list(label2id.keys()) # Recreate label_list from label2id for consistency\n",
        "\n",
        "    print(f\"Model and tokenizer loaded successfully.\")\n",
        "    print(f\"Loaded label mappings: {id2label}\")\n",
        "    print(f\"Loaded label_list: {label_list}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LIME for Token Classification (MAIN INTERPRETATION)\n",
        "\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import numpy as np\n",
        "import torch\n",
        "from IPython.core.display import display, HTML # For rich display\n",
        "\n",
        "if 'model' not in locals() or 'tokenizer' not in locals() or 'id2label' not in locals() or 'label_list' not in locals():\n",
        "    print(\"Model, tokenizer, or labels not loaded. Please run the 'Load Fine-tuned mBERT Model' cell first.\")\n",
        "else:\n",
        "    print(\"Starting LIME explanation...\")\n",
        "\n",
        "    # Let's use the same example sentence.\n",
        "    # From previous output:\n",
        "    # Original Text: 'ዋጋ፦ 1,300 ብር አድራሻ መገናኛ ስሪ ኤም ሲቲ ሞል ሁለተኛ ፎቅ'\n",
        "    # Predicted labels: ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
        "\n",
        "    example_text_lime = \"ዋጋ፦ 1,300 ብር አድራሻ መገናኛ ስሪ ኤም ሲቲ ሞል ሁለተኛ ፎቅ\"\n",
        "\n",
        "\n",
        "    # --- Define a prediction function for LIME ---\n",
        "    # LIME expects a function that takes a list of strings and returns a\n",
        "    # 2D numpy array of probabilities, where each row corresponds to a string\n",
        "    # and each column corresponds to a class/label.\n",
        "    # For NER, we adapt this by returning the maximum probability found for each label type\n",
        "    # across all tokens in the sentence. This allows LIME to function.\n",
        "    def predict_proba_lime(texts):\n",
        "        aggregated_probs = []\n",
        "        for text_single in texts:\n",
        "            inputs = tokenizer(text_single, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "            # Handle cases where input might become empty or too short after tokenization/truncation\n",
        "            if inputs['input_ids'].numel() < 2: # At least [CLS] and [SEP]\n",
        "                # If invalid, return a zeros array with the shape of (1, num_labels)\n",
        "                aggregated_probs.append(np.zeros(len(label_list)))\n",
        "                continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            logits = outputs.logits.squeeze(0) # (sequence_length, num_labels)\n",
        "\n",
        "            # Handle potential empty logits or incorrect dimensions\n",
        "            if logits.numel() == 0 or logits.dim() < 2 or logits.shape[1] != len(label_list):\n",
        "                 aggregated_probs.append(np.zeros(len(label_list)))\n",
        "                 continue\n",
        "\n",
        "            probabilities = torch.softmax(logits, dim=-1) # (sequence_length, num_labels)\n",
        "\n",
        "            # Take the maximum probability for each label type across all tokens.\n",
        "            # This aggregates token-level predictions to a sentence-level \"likelihood\" for LIME.\n",
        "            max_probs_per_label = torch.max(probabilities, dim=0).values.cpu().numpy()\n",
        "            aggregated_probs.append(max_probs_per_label)\n",
        "\n",
        "        return np.array(aggregated_probs)\n",
        "\n",
        "\n",
        "    # Initialize LIME explainer\n",
        "    # Use your label_list for class names so LIME knows which column maps to which label\n",
        "    explainer = LimeTextExplainer(class_names=label_list)\n",
        "\n",
        "    # --- Explain a prediction ---\n",
        "    # We will focus on one of the labels that was *predicted* by the model for this sentence: 'B-PRODUCT'.\n",
        "    # Or, we can choose a label we *expect* to see, like 'B-PRICE', to see what contributes to *not* predicting it, or to small probability.\n",
        "\n",
        "    # Let's try to explain why 'B-PRODUCT' was predicted, as it was one of the actual predictions.\n",
        "    target_label_name_lime = 'B-PRODUCT'\n",
        "    target_label_index_lime = label2id[target_label_name_lime]\n",
        "\n",
        "\n",
        "    print(f\"\\nExplaining prediction for label: '{target_label_name_lime}' (aggregated probability across sentence)\")\n",
        "    print(f\"Sentence: '{example_text_lime}'\")\n",
        "\n",
        "    # Generate explanation\n",
        "    # `num_features`: how many words to highlight in the explanation\n",
        "    # `num_samples`: how many perturbed samples LIME generates (higher = more stable, slower)\n",
        "    explanation_lime = explainer.explain_instance(\n",
        "        example_text_lime,\n",
        "        predict_proba_lime,\n",
        "        num_features=10,  # Top 10 most important words\n",
        "        num_samples=2000, # Number of perturbed samples (can increase for more stability, but slower)\n",
        "        labels=[target_label_index_lime] # Explain specifically this label's probability\n",
        "    )\n",
        "\n",
        "    # Display explanation in notebook\n",
        "    print(\"\\nLIME Explanation (HTML Output - green for positive contribution, red for negative):\")\n",
        "    display(HTML(explanation_lime.as_html(labels=[target_label_index_lime])))\n",
        "\n",
        "    print(\"\\nWords contributing to the prediction (Raw List):\")\n",
        "    # You can also get explanations for other labels by changing `label=...`\n",
        "    for word, weight in explanation_lime.as_list(label=target_label_index_lime):\n",
        "        print(f\"  '{word}': {weight:.4f}\")\n",
        "\n",
        "    print(\"\\nNote: LIME provides local explanations (for this specific sentence).\")\n",
        "    print(\"Green words in the HTML output indicate positive contribution to the prediction of the target label.\")\n",
        "    print(\"Red words indicate negative contribution.\")"
      ],
      "metadata": {
        "id": "2Zfhw7hIKNf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpPYSc75YWvB"
      },
      "outputs": [],
      "source": [
        "# SHAP for Token Classification\n",
        "\n",
        "import shap\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Ensure the model and tokenizer are loaded from the previous cell\n",
        "if 'model' not in locals() or 'tokenizer' not in locals():\n",
        "    print(\"Model or tokenizer not loaded. Please run the 'Load Fine-tuned mBERT Model' cell first.\")\n",
        "else:\n",
        "    print(\"Starting SHAP explanation (conceptual demonstration - REVISED & FIXED)...\")\n",
        "\n",
        "    # Example text for explanation\n",
        "    text = \"ዋጋ፦ 1,300 ብር አድራሻ መገናኛ ስሪ ኤም ሲቲ ሞል ሁለተኛ ፎቅ\"\n",
        "\n",
        "    # This function will take a list of text strings (perturbed by SHAP)\n",
        "    # and return the probabilities for a *specific target label* for a *specific target token*.\n",
        "    # This is a simplification to make SHAP runnable, as full NER explanation is complex.\n",
        "\n",
        "    # First, get the model's actual prediction for the example sentence\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Convert predicted IDs back to labels for display\n",
        "    predicted_labels = [id2label[p_id] for p_id in predictions]\n",
        "    subword_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze(0))\n",
        "\n",
        "    print(f\"\\nOriginal Text: '{text}'\")\n",
        "    print(f\"Subword Tokens: {subword_tokens}\")\n",
        "    print(f\"Predicted Labels: {predicted_labels}\")\n",
        "\n",
        "    # --- Choose a specific token and its predicted label to explain ---\n",
        "    # We will now explain why '[UNK]' at index 1 is predicted as 'B-PRODUCT',\n",
        "    # as this was an actual prediction from your model for this sentence.\n",
        "    target_token_str = '[UNK]' # The subword token string\n",
        "    target_label_name = 'B-PRODUCT' # The label it was predicted as\n",
        "\n",
        "    # Find the index of the target token and its corresponding label ID\n",
        "    target_token_index = None # Reset to None for fresh search\n",
        "    target_label_id = label2id[target_label_name]\n",
        "\n",
        "    # Iterate through the predictions to find the first instance of our target\n",
        "    for idx, (token, label) in enumerate(zip(subword_tokens, predicted_labels)):\n",
        "        if token == target_token_str and label == target_label_name:\n",
        "            target_token_index = idx\n",
        "            break # Found it, break the loop\n",
        "\n",
        "    if target_token_index is None:\n",
        "        print(f\"\\nCould not find '{target_token_str}' with '{target_label_name}' label in the example (even after retry).\")\n",
        "        print(\"This means the model didn't predict this combination in the given sample.\")\n",
        "        print(\"Please manually inspect the subword_tokens and predicted_labels to choose a target that *was* predicted.\")\n",
        "        print(\"Predicted Labels for reference: \", predicted_labels) # Print again for convenience\n",
        "    else:\n",
        "        print(f\"\\nAttempting to explain prediction for token '{subword_tokens[target_token_index]}' at index {target_token_index} as '{id2label[target_label_id]}'\")\n",
        "\n",
        "        # This `predictor` function is tailored for SHAP to explain a single output (probability of a specific label for a specific token)\n",
        "        def specific_token_label_predictor(texts_list):\n",
        "            probs_for_target = []\n",
        "            for text_val in texts_list:\n",
        "                inputs_val = tokenizer(text_val, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "                with torch.no_grad():\n",
        "                    outputs_val = model(**inputs_val)\n",
        "                logits_val = outputs_val.logits.squeeze(0) # Remove batch dim\n",
        "\n",
        "                # NEW ROBUST CHECK HERE:\n",
        "                # Check if logits_val is empty or if target_token_index is out of bounds\n",
        "                if logits_val.numel() == 0 or target_token_index >= logits_val.shape[0]:\n",
        "                    # Return 0.0 probability for the target label if the sequence is invalid or token is missing\n",
        "                    probs_for_target.append(0.0)\n",
        "                else:\n",
        "                    probs = torch.softmax(logits_val[target_token_index], dim=-1) # Probabilities for target token\n",
        "                    probs_for_target.append(probs[target_label_id].item()) # Probability of the specific target label\n",
        "\n",
        "            return np.array(probs_for_target).reshape(-1, 1) # SHAP expects 2D array (num_samples, num_outputs)\n",
        "\n",
        "\n",
        "        # Create a text masker for SHAP\n",
        "        masker = shap.maskers.Text(tokenizer.mask_token, collapse_mask_token=True)\n",
        "\n",
        "        # Initialize the SHAP Explainer\n",
        "        # Using `shap.Explainer` with the custom `specific_token_label_predictor`\n",
        "        explainer = shap.Explainer(specific_token_label_predictor, masker)\n",
        "\n",
        "        # Generate SHAP values for the original text\n",
        "        # This can take a moment depending on num_samples in explainer and text length\n",
        "        print(f\"Generating SHAP values for '{text}'...\")\n",
        "        shap_values = explainer([text]) # Pass the text in a list\n",
        "\n",
        "        # Print some info about the SHAP values\n",
        "        print(f\"\\nSHAP Values generated. Structure for visualization might vary.\")\n",
        "        print(f\"Base Value (expected prediction without any features): {shap_values.base_values[0]:.4f}\")\n",
        "        print(\"Word contributions (values indicate impact on the prediction's log-odds for the target label):\")\n",
        "\n",
        "        # For text data, shap_values.data holds the tokenized words, shap_values.values hold the contributions.\n",
        "        # Ensure we don't go out of bounds if SHAP's output shape is unexpected\n",
        "        if shap_values.data.ndim > 1 and shap_values.values.ndim > 1 and shap_values.data.shape[1] == shap_values.values.shape[1]:\n",
        "            for i, word in enumerate(shap_values.data[0]): # Iterate through the words\n",
        "                if i < shap_values.values[0].shape[0]:\n",
        "                    print(f\"  '{word}': {shap_values.values[0][i][0]:.4f}\") # Display the first output's contribution\n",
        "        else:\n",
        "            print(\"SHAP values structure not directly suitable for simple word-by-word printout. Verify `shap_values.data` and `shap_values.values` shapes.\")\n",
        "            print(f\"shap_values.data.shape: {shap_values.data.shape}\")\n",
        "            print(f\"shap_values.values.shape: {shap_values.values.shape}\")\n",
        "\n",
        "\n",
        "        print(\"\\nNote: For advanced SHAP visualizations with text (like highlighting words in the sentence),\")\n",
        "        print(\"it often requires more complex integration or specialized `shap.plots.text` usage tailored for sequence models.\")\n",
        "        print(\"LIME often provides a more straightforward visual interpretation for token contributions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Taks 6: FinTech Vendor Scorecard for Micro-Lending**"
      ],
      "metadata": {
        "id": "1qNinN9eUZeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zj-V7aUIRH-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# --- 0. Ensure Model and Tokenizer are Loaded (from Task 5 setup) ---\n",
        "# This block assumes 'model', 'tokenizer', 'id2label', 'label2id', 'label_list'\n",
        "\n",
        "# re-run the model loading cell from Task 5.\n",
        "\n",
        "# Check if model/tokenizer are loaded. If not, provide a warning.\n",
        "if 'model' not in locals() or 'tokenizer' not in locals() or \\\n",
        "   'id2label' not in locals() or 'label2id' not in locals():\n",
        "    print(\"WARNING: mBERT model and tokenizer not found in current environment.\")\n",
        "    print(\"Please ensure you have run the 'Load Fine-tuned mBERT Model' cell from Task 5 before running this code.\")\n",
        "    # Exit or provide placeholder objects to prevent immediate errors for demonstration\n",
        "    # In a real scenario, you'd halt execution or load them here.\n",
        "    # For this script, we'll proceed with dummy objects if not found,\n",
        "    # but the NER part will not function correctly without actual model.\n",
        "    class DummyTokenizer:\n",
        "        def __call__(self, text, return_tensors, truncation, padding):\n",
        "            return {'input_ids': torch.tensor([[101, 102]]), 'attention_mask': torch.tensor([[1, 1]])}\n",
        "        def convert_ids_to_tokens(self, ids):\n",
        "            return [\"[CLS]\", \"[SEP]\"]\n",
        "    class DummyModel:\n",
        "        def __call__(self, input_ids, attention_mask):\n",
        "            return type('obj', (object,), {'logits': torch.randn(1, 2, 20)})() # Dummy logits\n",
        "        def eval(self): pass\n",
        "\n",
        "    tokenizer = DummyTokenizer()\n",
        "    model = DummyModel()\n",
        "    id2label = {0: \"O\", 1: \"B-PRODUCT\", 2: \"I-PRODUCT\", 3: \"B-PRICE\", 4: \"I-PRICE\", 5: \"B-CURRENCY\", 6: \"I-CURRENCY\", 7: \"B-LOCATION\", 8: \"I-LOCATION\"}\n",
        "    label2id = {v: k for k, v in id2label.items()}\n",
        "    label_list = list(label2id.keys())\n",
        "    print(\"Using dummy model/tokenizer for demonstration. NER extraction will not be functional.\")\n",
        "\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing\n",
        "\n",
        "data_raw = [\n",
        "    {\"channel_name\": \"EthioTechMart\", \"post_id\": 1, \"text\": \"ላፕቶፕ Dell XPS 13 ዋጋ: 45,000 ብር. አድራሻ: አዲስ አበባ, ቦሌ.\", \"views\": 1200, \"timestamp\": \"2025-06-20T10:00:00Z\"},\n",
        "    {\"channel_name\": \"EthioTechMart\", \"post_id\": 2, \"text\": \"አዲስ ሳምሰንግ ስልክ ጋላክሲ S23 ዋጋ: 30,000 ብር.\", \"views\": 1500, \"timestamp\": \"2025-06-21T14:30:00Z\"},\n",
        "    {\"channel_name\": \"EthioTechMart\", \"post_id\": 3, \"text\": \"ገራሚ ጌም ኮንሶል Playstation 5 ዋጋ: 70,000 ብር.\", \"views\": 2000, \"timestamp\": \"2025-06-22T09:15:00Z\"},\n",
        "    {\"channel_name\": \"EthioTechMart\", \"post_id\": 4, \"text\": \"ዋጋ፦ 1,300 ብር አድራሻ መገናኛ ስሪ ኤም ሲቲ ሞል ሁለተኛ ፎቅ\", \"views\": 500, \"timestamp\": \"2025-06-18T11:00:00Z\"}, # This is our difficult case from Task 5\n",
        "    {\"channel_name\": \"EthioTechMart\", \"post_id\": 5, \"text\": \"ሽያጭ! የዳቦ መጋገሪያ ማሽን ዋጋ: 5,500 ETB. ይገኛል ሜክሲኮ አደባባይ.\", \"views\": 900, \"timestamp\": \"2025-06-15T18:00:00Z\"},\n",
        "    {\"channel_name\": \"AddisElectronics\", \"post_id\": 6, \"text\": \"ለሽያጭ: Sony Camera Alpha 7 III. ዋጋ: 120,000 ብር. ሱቃችን ፒያሳ ነው\", \"views\": 3000, \"timestamp\": \"2025-06-19T12:00:00Z\"},\n",
        "    {\"channel_name\": \"AddisElectronics\", \"post_id\": 7, \"text\": \"አይፎን 14 ፕሮ ማክስ በ24,500 ብር ብቻ! አድራሻ: ካዛንቺስ.\", \"views\": 2800, \"timestamp\": \"2025-06-20T16:00:00Z\"},\n",
        "    {\"channel_name\": \"AddisElectronics\", \"post_id\": 8, \"text\": \"ቲቪ Samsung 55 ኢንች ዋጋ: 40,000 ብር. ቦሌ በሚገኘው መጋዘን\", \"views\": 2500, \"timestamp\": \"2025-06-22T10:30:00Z\"},\n",
        "    {\"channel_name\": \"AddisHomeGoods\", \"post_id\": 9, \"text\": \"የተለያዩ የወጥ ቤት እቃዎች. ዋጋ: 2,500 ብር ጀምሮ.\", \"views\": 700, \"timestamp\": \"2025-06-17T08:00:00Z\"},\n",
        "    {\"channel_name\": \"AddisHomeGoods\", \"post_id\": 10, \"text\": \"አዲስ የመኝታ ክፍል እቃዎች ዋጋ: 18,000 ብር.\", \"views\": 650, \"timestamp\": \"2025-06-18T15:00:00Z\"},\n",
        "    {\"channel_name\": \"EthioTechMart\", \"post_id\": 11, \"text\": \"የጨዋታ ጆሮ ማዳመጫዎች (Gaming Headphones) በ1,800 ብር ብቻ!\", \"views\": 1050, \"timestamp\": \"2025-06-19T13:00:00Z\"},\n",
        "    {\"channel_name\": \"EthioTechMart\", \"post_id\": 12, \"text\": \"ብራንድ አዲስ የሰዓት ስልክ በ8,500 ብር!\", \"views\": 950, \"timestamp\": \"2025-06-23T11:00:00Z\"},\n",
        "    {\"channel_name\": \"EthioTechMart\", \"post_id\": 13, \"text\": \"Dell ላፕቶፕ - 35,000 ብር.\", \"views\": 1100, \"timestamp\": \"2025-06-16T09:00:00Z\"},\n",
        "    {\"channel_name\": \"AddisElectronics\", \"post_id\": 14, \"text\": \"ስማርት ዋች በ10,000 ብር.\", \"views\": 2200, \"timestamp\": \"2025-06-21T10:00:00Z\"},\n",
        "    {\"channel_name\": \"AddisHomeGoods\", \"post_id\": 15, \"text\": \"የአትክልት መሳሪያዎች: 700 ብር.\", \"views\": 500, \"timestamp\": \"2025-06-20T17:00:00Z\"},\n",
        "]\n",
        "\n",
        "df_posts = pd.DataFrame(data_raw)\n",
        "df_posts['timestamp'] = pd.to_datetime(df_posts['timestamp'])\n"
      ],
      "metadata": {
        "id": "r55cZcQoRIg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 2. NER Entity Extraction Function ---\n",
        "\n",
        "def extract_entities_from_text(text, tokenizer, model, id2label):\n",
        "    \"\"\"\n",
        "    Extracts entities (PRODUCT, PRICE, CURRENCY, LOCATION) from a given text\n",
        "    using the fine-tuned NER model.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze(0))\n",
        "\n",
        "    entities = {\n",
        "        'PRODUCT': [],\n",
        "        'PRICE': [],\n",
        "        'CURRENCY': [],\n",
        "        'LOCATION': []\n",
        "    }\n",
        "\n",
        "    current_entity = {\"text\": [], \"type\": None}\n",
        "\n",
        "    for token, pred_id in zip(tokens, predictions):\n",
        "        label = id2label[pred_id]\n",
        "\n",
        "        # Skip special tokens and unknown tokens if they don't form part of a recognized entity\n",
        "        if token in tokenizer.all_special_tokens or token.startswith(\"##\") or token == '[UNK]':\n",
        "            if label == 'O' and current_entity[\"type\"] is None:\n",
        "                continue # Skip if it's outside and not part of an ongoing entity\n",
        "            # If it's a subword or UNK and part of an ongoing entity, include it in text\n",
        "            if current_entity[\"type\"] is not None:\n",
        "                current_entity[\"text\"].append(token.replace(\"##\", \"\")) # Remove ## for readability\n",
        "\n",
        "        elif label.startswith(\"B-\"):\n",
        "            # If there was a previous entity, save it\n",
        "            if current_entity[\"type\"] is not None and current_entity[\"text\"]:\n",
        "                entity_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(current_entity[\"text\"]), skip_special_tokens=True).strip()\n",
        "                # entity_text = \"\".join(current_entity[\"text\"]).strip() # Simpler string join for subwords\n",
        "                entities[current_entity[\"type\"]].append(entity_text)\n",
        "\n",
        "            # Start a new entity\n",
        "            entity_type = label[2:]\n",
        "            current_entity = {\"text\": [token], \"type\": entity_type}\n",
        "\n",
        "        elif label.startswith(\"I-\"):\n",
        "            if current_entity[\"type\"] is not None and label[2:] == current_entity[\"type\"]:\n",
        "                current_entity[\"text\"].append(token)\n",
        "            else:\n",
        "                # Malformed sequence (I- without B- or type mismatch), treat as new B- or O\n",
        "                if current_entity[\"type\"] is not None and current_entity[\"text\"]:\n",
        "                    entity_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(current_entity[\"text\"]), skip_special_tokens=True).strip()\n",
        "                    entities[current_entity[\"type\"]].append(entity_text)\n",
        "\n",
        "                entity_type = label[2:]\n",
        "                current_entity = {\"text\": [token], \"type\": entity_type} # Start as if it was a B-\n",
        "\n",
        "        else: # label == 'O'\n",
        "            if current_entity[\"type\"] is not None and current_entity[\"text\"]:\n",
        "                entity_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(current_entity[\"text\"]), skip_special_tokens=True).strip()\n",
        "                entities[current_entity[\"type\"]].append(entity_text)\n",
        "            current_entity = {\"text\": [], \"type\": None}\n",
        "\n",
        "    # Don't forget to save the last entity if loop ends\n",
        "    if current_entity[\"type\"] is not None and current_entity[\"text\"]:\n",
        "        entity_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(current_entity[\"text\"]), skip_special_tokens=True).strip()\n",
        "        entities[current_entity[\"type\"]].append(entity_text)\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Apply NER to all posts\n",
        "df_posts['extracted_entities'] = df_posts['text'].apply(\n",
        "    lambda x: extract_entities_from_text(x, tokenizer, model, id2label)\n",
        ")\n",
        "\n",
        "print(\"Entities extracted for sample posts:\")\n",
        "print(df_posts[['text', 'extracted_entities']].head())\n"
      ],
      "metadata": {
        "id": "gZc8tYAbRkDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Helper Functions for Metric Calculation ---\n",
        "\n",
        "def calculate_posting_frequency(df_vendor_posts):\n",
        "    \"\"\"Calculates average posts per week for a vendor.\"\"\"\n",
        "    if df_vendor_posts.empty:\n",
        "        return 0.0\n",
        "\n",
        "    # Ensure timestamp is datetime type\n",
        "    df_vendor_posts = df_vendor_posts.sort_values('timestamp')\n",
        "\n",
        "    # Calculate duration in weeks\n",
        "    if len(df_vendor_posts) == 1:\n",
        "        # If only one post, assume a frequency of 1 post/week for simplicity\n",
        "        # or handle as 'insufficient data'\n",
        "        return 1.0 # Or np.nan\n",
        "\n",
        "    min_date = df_vendor_posts['timestamp'].min()\n",
        "    max_date = df_vendor_posts['timestamp'].max()\n",
        "\n",
        "    time_span_days = (max_date - min_date).days\n",
        "\n",
        "    if time_span_days == 0: # All posts on the same day\n",
        "        return len(df_vendor_posts) * 7.0 # Posts per day * 7\n",
        "\n",
        "    posting_frequency = (len(df_vendor_posts) / time_span_days) * 7\n",
        "    return posting_frequency\n",
        "\n",
        "def parse_price(price_str):\n",
        "    \"\"\"\n",
        "    Parses a price string (e.g., '1,300', '45.000', '5,500 ETB') into a float.\n",
        "    Handles Amharic numerals if present. Assumes numbers are mostly Western digits but\n",
        "    adds a basic Amharic digit mapping for robustness.\n",
        "    \"\"\"\n",
        "    if not isinstance(price_str, str):\n",
        "        return np.nan\n",
        "\n",
        "    # Amharic to Latin digit mapping (basic)\n",
        "    amharic_digits = {\n",
        "        '፩': '1', '፪': '2', '፫': '3', '፬': '4', '፭': '5',\n",
        "        '፮': '6', '፯': '7', '፰': '8', '፱': '9', '፰': '0' # Note: ፰ is 8, no explicit 0 in traditional Ethiopian numerals but for general text we might see it.\n",
        "    }\n",
        "    # For more common modern Amharic numeric strings, it's usually Western digits.\n",
        "    # We will primarily focus on Western digits as they are common in digital contexts.\n",
        "\n",
        "    cleaned_price = price_str.lower().replace(\"ብር\", \"\").replace(\"etb\", \"\").strip()\n",
        "\n",
        "    # Replace Amharic digits with Latin digits\n",
        "    for am_digit, lat_digit in amharic_digits.items():\n",
        "        cleaned_price = cleaned_price.replace(am_digit, lat_digit)\n",
        "\n",
        "    # Remove commas (used as thousands separators) and non-numeric characters except period\n",
        "    cleaned_price = re.sub(r'[^\\d.]', '', cleaned_price)\n",
        "\n",
        "    try:\n",
        "        return float(cleaned_price)\n",
        "    except ValueError:\n",
        "        return np.nan # Return NaN if conversion fails\n"
      ],
      "metadata": {
        "id": "H1ovNS7ZR0lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Vendor Analytics Engine ---\n",
        "\n",
        "def develop_vendor_analytics_engine(df_posts):\n",
        "    \"\"\"\n",
        "    Processes all posts, extracts entities, and calculates key performance metrics for each vendor.\n",
        "    \"\"\"\n",
        "    vendor_metrics = {}\n",
        "\n",
        "    for channel_name, df_vendor_posts in df_posts.groupby('channel_name'):\n",
        "        print(f\"\\nProcessing vendor: {channel_name}\")\n",
        "\n",
        "        # Initialize metrics for the current vendor\n",
        "        total_posts = len(df_vendor_posts)\n",
        "        total_views = df_vendor_posts['views'].sum()\n",
        "\n",
        "        # Market Reach & Engagement\n",
        "        avg_views_per_post = total_views / total_posts if total_posts > 0 else 0\n",
        "\n",
        "        top_post = df_vendor_posts.loc[df_vendor_posts['views'].idxmax()] if not df_vendor_posts.empty else None\n",
        "        top_post_product = None\n",
        "        top_post_price = np.nan\n",
        "\n",
        "        if top_post is not None:\n",
        "            top_post_entities = top_post['extracted_entities']\n",
        "            if top_post_entities.get('PRODUCT'):\n",
        "                top_post_product = top_post_entities['PRODUCT'][0] # Take first product if multiple\n",
        "            if top_post_entities.get('PRICE'):\n",
        "                # Try to parse the price from the top post\n",
        "                raw_price = top_post_entities['PRICE'][0]\n",
        "                top_post_price = parse_price(raw_price)\n",
        "\n",
        "        # Activity & Consistency\n",
        "        posting_frequency = calculate_posting_frequency(df_vendor_posts)\n",
        "\n",
        "        # Business Profile (Average Price Point)\n",
        "        all_prices = []\n",
        "        for entities_dict in df_vendor_posts['extracted_entities']:\n",
        "            if entities_dict.get('PRICE'):\n",
        "                for price_str in entities_dict['PRICE']:\n",
        "                    parsed_price = parse_price(price_str)\n",
        "                    if not np.isnan(parsed_price):\n",
        "                        all_prices.append(parsed_price)\n",
        "\n",
        "        average_price_point = np.mean(all_prices) if all_prices else np.nan\n",
        "\n",
        "        # Combine metrics into a simple, weighted \"Lending Score\"\n",
        "        # Example weighting: (Average Views * 0.4) + (Posting Frequency * 0.3) + (Average Price Point / Max Price * 0.3)\n",
        "        # Assuming higher price point is better, but needs normalization.\n",
        "        # Let's normalize views and frequency by their max across all vendors for fairness if we were comparing broadly.\n",
        "\n",
        "\n",
        "\n",
        "        vendor_metrics[channel_name] = {\n",
        "            \"Total Posts\": total_posts,\n",
        "            \"Posting Frequency (Posts/Week)\": posting_frequency,\n",
        "            \"Average Views per Post\": avg_views_per_post,\n",
        "            \"Top Performing Post\": top_post['text'] if top_post is not None else \"N/A\",\n",
        "            \"Top Post Product\": top_post_product if top_post_product else \"N/A\",\n",
        "            \"Top Post Price (ETB)\": top_post_price,\n",
        "            \"Average Price Point (ETB)\": average_price_point,\n",
        "        }\n",
        "\n",
        "    return pd.DataFrame.from_dict(vendor_metrics, orient='index')\n",
        "\n",
        "# Run the analytics engine\n",
        "df_vendor_analytics = develop_vendor_analytics_engine(df_posts.copy()) # Use a copy to avoid modifying original\n",
        "\n",
        "print(\"\\n--- Vendor Analytics Summary (Raw Metrics) ---\")\n",
        "print(df_vendor_analytics)"
      ],
      "metadata": {
        "id": "xyJuA4l1SfQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Create a Final \"Lending Score\" and Vendor Scorecard ---\n",
        "\n",
        "def create_lending_scorecard(df_analytics):\n",
        "    \"\"\"\n",
        "    Calculates a weighted lending score and presents a summary scorecard.\n",
        "    \"\"\"\n",
        "    df_scorecard = df_analytics.copy()\n",
        "\n",
        "    # Normalize metrics for scoring (simple min-max scaling for demonstration)\n",
        "    # Ensure no division by zero if max is 0\n",
        "    max_views = df_scorecard['Average Views per Post'].max()\n",
        "    df_scorecard['Normalized Views'] = df_scorecard['Average Views per Post'] / max_views if max_views > 0 else 0\n",
        "\n",
        "    max_frequency = df_scorecard['Posting Frequency (Posts/Week)'].max()\n",
        "    df_scorecard['Normalized Frequency'] = df_scorecard['Posting Frequency (Posts/Week)'] / max_frequency if max_frequency > 0 else 0\n",
        "\n",
        "    # For average price, decide if higher or lower is better for lending.\n",
        "    # Assuming higher price point indicates potentially higher value transactions.\n",
        "    # Handle NaN values for max_price to avoid errors.\n",
        "    prices_not_nan = df_scorecard['Average Price Point (ETB)'].dropna()\n",
        "    max_price = prices_not_nan.max() if not prices_not_nan.empty else 1.0 # Avoid division by zero\n",
        "    df_scorecard['Normalized Price'] = df_scorecard['Average Price Point (ETB)'] / max_price if max_price > 0 else 0\n",
        "    # Fill NaN normalized prices with 0 if original was NaN\n",
        "    df_scorecard['Normalized Price'] = df_scorecard['Normalized Price'].fillna(0)\n",
        "\n",
        "\n",
        "    # Define weights (can be adjusted based on business priorities)\n",
        "    # Example: Views are 50% important, Frequency 30%, Price 20%\n",
        "    weight_views = 0.5\n",
        "    weight_frequency = 0.3\n",
        "    weight_price = 0.2\n",
        "\n",
        "    # Calculate Lending Score\n",
        "    df_scorecard['Lending Score'] = (\n",
        "        df_scorecard['Normalized Views'] * weight_views +\n",
        "        df_scorecard['Normalized Frequency'] * weight_frequency +\n",
        "        df_scorecard['Normalized Price'] * weight_price\n",
        "    )\n",
        "\n",
        "    # Select and rename columns for the final report table\n",
        "    final_columns = {\n",
        "        'Average Views per Post': 'Avg. Views/Post',\n",
        "        'Posting Frequency (Posts/Week)': 'Posts/Week',\n",
        "        'Average Price Point (ETB)': 'Avg. Price (ETB)',\n",
        "        'Lending Score': 'Lending Score'\n",
        "    }\n",
        "\n",
        "    df_final_scorecard = df_scorecard[list(final_columns.keys())].rename(columns=final_columns)\n",
        "\n",
        "    return df_final_scorecard.sort_values('Lending Score', ascending=False)\n",
        "\n",
        "# Generate and display the final scorecard\n",
        "final_scorecard_df = create_lending_scorecard(df_vendor_analytics)\n",
        "\n",
        "print(\"\\n--- Final Vendor Scorecard for Micro-Lending ---\")\n",
        "print(final_scorecard_df.to_markdown(index=True)) # Use to_markdown for easy display in markdown format\n",
        "print(\"\\nNote: The 'Lending Score' is a composite metric based on weighted normalized values.\")\n",
        "print(\"Weights used: Average Views (0.5), Posting Frequency (0.3), Average Price Point (0.2).\")\n",
        "print(\"These weights can be adjusted based on EthioMart's specific lending criteria.\")\n"
      ],
      "metadata": {
        "id": "T1tPMnrnSn5q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyN+xEo9TBcbQ8giTg2E7ioa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}