{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7492c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# scripts/text_preprocessor.py\n",
    "import re\n",
    "import unicodedata\n",
    "from scripts.config import REMOVE_STOPWORDS # Import config for stopword setting\n",
    "\n",
    "class AmharicPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing Amharic text data.\n",
    "    Handles cleaning, normalization, and tokenization.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Define homophone mapping for normalization. This is a complex area.\n",
    "        # For NER, sometimes preserving original form is better, but for general text tasks, normalization helps.\n",
    "        # A more robust solution would involve a lookup table or more complex linguistic rules.\n",
    "        # For now, a conservative approach is taken for NER, primarily cleaning and standardizing.\n",
    "        # If true homophone normalization is needed, this mapping should be extensively researched.\n",
    "        # Example of a few common variations, focus on visual consistency for now:\n",
    "        self.homophone_map = {\n",
    "            '·àÄ': '·àÄ', '·àÉ': '·àÄ', '·àê': '·àÄ', '·àì': '·àÄ', '·äÄ': '·àÄ', '·äÉ': '·àÄ', '·äª': '·àÄ',\n",
    "            '·à∞': '·à∞', '·à†': '·à∞',\n",
    "            '·å∏': '·å∏', '·çÄ': '·å∏',\n",
    "            '·ãç': '·ãç', '·ãâ': '·ãç', # More of a spelling variant\n",
    "            '·ä†': '·ä†', '·ä£': '·ä†', '·ãê': '·ä†', '·ãì': '·ä†',\n",
    "            # Add other known variations if critical for NER, else LLM might handle.\n",
    "        }\n",
    "\n",
    "        # Common Amharic punctuation and symbols to remove or replace.\n",
    "        # Includes Ethiopic punctuation marks.\n",
    "        self.amharic_punctuation = r'[\\!\\@\\#\\$\\%\\^\\¬´\\¬ª\\&\\*\\(\\)\\‚Ä¶\\[\\]\\{\\}\\;‚Äú‚Äü‚Äù‚Ä∫‚Äò‚Äô\\\"\\'\\:\\,\\.\\‚Äπ\\/\\<\\>\\?\\‚Äî\\\\\\`\\¬¥\\~\\|\\=\\+\\·ç°\\·ç¢\\·ç§\\·ç•\\·çß\\·ç®\\·ç†]'\n",
    "\n",
    "        # Regex for common emojis and symbols\n",
    "        self.emojis_and_symbols = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U000024C2-\\U0001F251\"\n",
    "            \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "            \"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "            \"\\U0000FE00-\\U0000FE0F\"  # Variation Selectors\n",
    "            \"\\U0001FAD0-\\U0001FADF\"  # Added in Unicode 13.0 for food/drink\n",
    "            \"\\U0001F6F0-\\U0001F6FF\"  # Symbols and Pictographs Extended-A\n",
    "            \"]+\", flags=re.UNICODE\n",
    "        )\n",
    "\n",
    "        # Amharic stop words - this list needs to be comprehensive.\n",
    "        # This is a sample; for production, source a larger list or create one.\n",
    "        self.amharic_stopwords = set([\n",
    "            \"·ä•·äì\", \"·äê·â†·à≠\", \"·äê·ãç\", \"·åç·äï\", \"·ä®\", \"·â†\", \"·àã·ã≠\", \"·ãà·ã∞\", \"·à≤·àç\", \"·àà\", \"·ã®\", \"·ä†·ãé\", \"·ä†·ã≠·ã∞·àà·àù\", \"·ã´·àà\",\n",
    "            \"·ä†·àâ·âµ\", \"·ä†·àã·â∏·ãç\", \"·ä†·àâ\", \"·à≤·àÜ·äï\", \"·ã®·àÜ·äê\", \"·àÜ·äñ\", \"·â†·çä·âµ\", \"·ä®·ãõ\", \"·â†·äã·àã\", \"·ãõ·à¨\", \"·âµ·äì·äï·âµ\",\n",
    "            \"·äê·åà\", \"·ä†·äï·ã≥·äï·ãµ\", \"·â•·ãô\", \"·àÅ·àâ·äï·àù\", \"·àÅ·àâ·àù\", \"·àõ·äï\", \"·àù·äï\", \"·ã®·âµ\", \"·ä•·äï·ã¥·âµ\", \"·àà·àù·äï\", \"·àò·âº\",\n",
    "            \"·ã®·âµ·äõ·ãâ\", \"·ä•·à±\", \"·ä•·à∑\", \"·ä•·äê·à±\", \"·ä•·äõ\", \"·ä†·äï·â∞\", \"·ä†·äï·â∫\", \"·ä•·äì·äï·â∞\", \"·ä•·äî\", \"·ä•·à≠·à±\", \"·ä•·à≠·à∑\", \"·ä•·à≠·à≥·â∏·ãç\",\n",
    "            \"·ã´\", \"·ã≠·àÖ\", \"·ä•·äê·ãö·àÖ\", \"·ä•·äê·ãö·ã´\", \"·ä•·äï·ã≤·àÅ·àù\", \"·ãà·ã≠·àù\", \"·àù·äì·àç·â£·âµ\", \"·â•·âª\", \"·åç·ã¥·â≥\", \"·ä†·äï·ã¥\", \"·àÅ·àà·âµ\",\n",
    "            \"·à∂·àµ·âµ\", \"·ä†·à´·âµ\", \"·ä†·àù·àµ·âµ\", \"·àµ·ãµ·àµ·âµ\", \"·à∞·â£·âµ\", \"·àµ·àù·äï·âµ\", \"·ãò·å†·äù\", \"·ä†·àµ·à≠\"\n",
    "            # Add more as you refine your stopwords list\n",
    "        ])\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Removes URLs, mentions, hashtags, non-Amharic characters, emojis, and extra whitespace.\"\"\"\n",
    "        if not isinstance(text, str): # Handle non-string inputs (e.g., None)\n",
    "            return \"\"\n",
    "        # 1. Decode potential HTML entities (if any are left from source)\n",
    "        text = unicodedata.normalize('NFKC', text)\n",
    "        # 2. Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        # 3. Remove mentions and hashtags\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        # 4. Remove emojis\n",
    "        text = self.emojis_and_symbols.sub(r'', text)\n",
    "        # 5. Remove non-Amharic script (Ethiopic Unicode blocks), except numbers and some common symbols\n",
    "        # Unicode ranges for Ethiopic: U+1200‚ÄìU+137F, U+2D80‚ÄìU+2DDF (Ethiopic Extended), U+AB00‚ÄìU+AB2F (Ethiopic Extended-A)\n",
    "        # Keeping basic Latin letters for potential loanwords/brands, and common numbers.\n",
    "        # A more strict approach would remove all non-Ethiopic script except numbers.\n",
    "        text = re.sub(r'[^\\u1200-\\u137F\\u2D80-\\u2DDF\\uAB00-\\uAB2F0-9a-zA-Z\\s' + re.escape(self.amharic_punctuation) + ']', '', text)\n",
    "        # 6. Remove excess spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def normalize_amharic_characters(self, text):\n",
    "        \"\"\"Applies homophone normalization if defined. Cautious approach for NER.\"\"\"\n",
    "        normalized_text = \"\"\n",
    "        for char in text:\n",
    "            # Only apply if a mapping exists. Otherwise, keep original.\n",
    "            normalized_text += self.homophone_map.get(char, char)\n",
    "        return normalized_text\n",
    "\n",
    "    def normalize_numbers(self, text):\n",
    "        \"\"\"Converts Ge'ez numbers to Arabic numerals.\"\"\"\n",
    "        geez_to_arabic_map = {\n",
    "            '·ç©': '1', '·ç™': '2', '·ç´': '3', '·ç¨': '4', '·ç≠': '5',\n",
    "            '·çÆ': '6', '·çØ': '7', '·ç∞': '8', '·ç±': '9', '·ç≤': '10',\n",
    "            '·ç≥': '20', '·ç¥': '30', '·çµ': '40', '·ç∂': '50', '·ç∑': '60',\n",
    "            '·ç∏': '70', '·çπ': '80', '·ç∫': '90', '·çª': '100', '·çº': '10000'\n",
    "        }\n",
    "        for geez, arabic in geez_to_arabic_map.items():\n",
    "            text = text.replace(geez, arabic)\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Basic tokenization by spaces and common punctuation.\n",
    "           For production, consider dedicated Amharic tokenizers if available and performing better.\n",
    "        \"\"\"\n",
    "        # Split by spaces and by standard Amharic punctuation, keeping punctuation as tokens\n",
    "        # Example: \"·âÉ·àç::\" -> [\"·âÉ·àç\", \"::\"] or \"·âÉ·àç\", \".\", \".\"\n",
    "        # For now, just split by whitespace and remove empty strings.\n",
    "        tokens = text.split()\n",
    "        return [token for token in tokens if token]\n",
    "\n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Removes defined Amharic stopwords from a list of tokens.\"\"\"\n",
    "        return [token for token in tokens if token not in self.amharic_stopwords]\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Main preprocessing pipeline for Amharic text.\n",
    "        Applies cleaning, normalization, tokenization, and optional stopword removal.\n",
    "        \"\"\"\n",
    "        # 1. Clean the raw text (remove noise)\n",
    "        text = self.clean_text(text)\n",
    "        # 2. Normalize Amharic characters (homophones etc.)\n",
    "        text = self.normalize_amharic_characters(text)\n",
    "        # 3. Normalize numbers (Ge'ez to Arabic)\n",
    "        text = self.normalize_numbers(text)\n",
    "        # 4. Tokenize the text\n",
    "        tokens = self.tokenize(text)\n",
    "\n",
    "        # 5. Optional: Remove stopwords. For NER, it's often better to keep them for context.\n",
    "        if REMOVE_STOPWORDS:\n",
    "            tokens = self.remove_stopwords(tokens)\n",
    "\n",
    "        # Join tokens back into a string. For NER, sequence is important.\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Simple test cases for the preprocessor\n",
    "    preprocessor = AmharicPreprocessor()\n",
    "\n",
    "    print(\"--- Testing AmharicPreprocessor ---\")\n",
    "\n",
    "    test_texts = [\n",
    "        \"·å§·äì ·ã≠·àµ·å•·àç·äù! ·ã≠·àÖ ·àù·à≠·å• ·ã®·àû·â£·ã≠·àç ·àµ·àç·ä≠ ·â†1500 ·â•·à≠ ·â•·âª ·ã≠·åà·äõ·àç·ç¢ ·ä†·ãµ·à´·àª: ·ä†·ã≤·àµ ·ä†·â†·â£·ç¢ @exampleuser #·àà·àΩ·ã´·å≠\",\n",
    "        \"·ã®·â∞·àà·ã´·ã© ·ä•·âÉ·ãé·âΩ ·ä†·àâ·äï·ç¢ ·ãõ·à¨ ·â†5000 ·â•·à≠ ·ä•·åÖ·åç ·â†·å£·àù ·å•·à© ·àã·çï·â∂·çï·ç¢ ·ä†·ãµ·à´·àª·âΩ·äï ·åé·â∞·à´·ç¢\",\n",
    "        \"·ã≠·àÖ ·å´·àõ ·â†·çª ·â•·à≠ ·ã≠·åà·äõ·àç·ç¢ ·ä†·ãµ·à´·àª - ·àµ·ãµ·àµ·âµ ·ä™·àé·ç¢ www.example.com\",\n",
    "        \"·å•·à© ·ãã·åã: 250 ·â•·à≠ + 50 ·â•·à≠ Delivery Fee üööüì¶\",\n",
    "        \"·ã®·àà·àù! ·àò·à®·åÉ ·ã®·àà·àù·ç¢\"\n",
    "    ]\n",
    "\n",
    "    for i, text in enumerate(test_texts):\n",
    "        print(f\"\\nOriginal {i+1}: {text}\")\n",
    "        preprocessed_text = preprocessor.preprocess(text)\n",
    "        print(f\"Preprocessed {i+1}: {preprocessed_text}\")\n",
    "\n",
    "    # Test with REMOVE_STOPWORDS = True (temporarily override for demonstration)\n",
    "    # Ensure to reset it if needed for main pipeline\n",
    "    from scripts.config import REMOVE_STOPWORDS as initial_remove_stopwords\n",
    "    temp_preprocessor = AmharicPreprocessor()\n",
    "    temp_preprocessor.remove_stopwords = True # Directly set for testing\n",
    "    print(\"\\n--- Testing with Stopwords Removed (Temporary Override) ---\")\n",
    "    preprocessed_with_stopwords_removed = temp_preprocessor.preprocess(test_texts[0])\n",
    "    print(f\"Original: {test_texts[0]}\")\n",
    "    print(f\"Preprocessed (Stopwords Removed): {preprocessed_with_stopwords_removed}\")\n",
    "    # Reset to original config value\n",
    "    from scripts.config import REMOVE_STOPWORDS\n",
    "    REMOVE_STOPWORDS = initial_remove_stopwords"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
