{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evtH6U9-lkkk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate seqeval -q\n",
        "!pip install optimum -q # Optional: For ONNX export or quantization later"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval -q"
      ],
      "metadata": {
        "id": "n65Ks2n6rcdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n2. Importing libraries and loading model components...\")\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "from datasets import load_dataset, Dataset, Features, Value, ClassLabel, Sequence\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the model checkpoint\n",
        "# Choose one of the following models by uncommenting it:\n",
        "print(\"\\n2. Importing libraries and loading model components...\")\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "from datasets import load_dataset, Dataset, Features, Value, ClassLabel, Sequence\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the model checkpoint\n",
        "# Choose one of the following models by uncommenting it:\n",
        "# MODEL_CHECKPOINT = \"xlm-roberta-base\"        # Strong general-purpose multilingual model\n",
        "# MODEL_CHECKPOINT = \"attributio/bert-tiny-amharic\" # Smaller, faster, Amharic-specific\n",
        "MODEL_CHECKPOINT = \"bert-base-multilingual-cased\"  # Good multilingual model for African languages\n",
        "\n",
        "# Load tokenizer\n",
        "print(f\"Loading tokenizer from: {MODEL_CHECKPOINT}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Define your labels - these MUST match exactly with your CoNLL labels\n",
        "# IMPORTANT: Ensure this list contains all unique B-I-O tags from your labeled_telegram_product_price_location.txt\n",
        "label_list = [\n",
        "    \"O\",\n",
        "    \"B-PRODUCT\", # Changed P to uppercase\n",
        "    \"I-PRODUCT\", # Changed P to uppercase\n",
        "    \"B-PRICE\",\n",
        "    \"I-PRICE\",\n",
        "    \"B-LOC\",\n",
        "    \"I-LOC\"\n",
        "]\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "print(f\"Defined labels: {label_list}\")\n",
        "print(f\"id2label mapping: {id2label}\")\n",
        "print(f\"label2id mapping: {label2id}\")\n",
        "print(f\"Loading tokenizer from: {MODEL_CHECKPOINT}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Define your labels - these MUST match exactly with your CoNLL labels\n",
        "# IMPORTANT: Ensure this list contains all unique B-I-O tags from your labeled_telegram_product_price_location.txt\n",
        "label_list = [\n",
        "    \"O\",\n",
        "    \"B-PRODUCT\", # Changed P to uppercase\n",
        "    \"I-PRODUCT\", # Changed P to uppercase\n",
        "    \"B-PRICE\",\n",
        "    \"I-PRICE\",\n",
        "    \"B-LOC\",\n",
        "    \"I-LOC\"\n",
        "]\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "print(f\"Defined labels: {label_list}\")\n",
        "print(f\"id2label mapping: {id2label}\")\n",
        "print(f\"label2id mapping: {label2id}\")"
      ],
      "metadata": {
        "id": "95ddQUr1rN5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n3. Loading the labeled dataset...\")\n",
        "\n",
        "# Option A: Upload directly to Colab (Temporary - for small files)\n",
        "# Run this cell, a file uploader will appear. Select your .txt file.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "uploaded_file_name = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {uploaded_file_name}\")\n",
        "file_name = uploaded_file_name # Use the uploaded file name\n",
        "\n",
        "# # Option B: Mount Google Drive (Recommended - for persistent storage)\n",
        "# # Uncomment the following lines if you want to use Google Drive and have your file there.\n",
        "# # from google.colab import drive\n",
        "# # drive.mount('/content/drive')\n",
        "# # # Adjust this path to where you saved your file in Google Drive\n",
        "# # # Example: if it's in a folder named 'my_project' in the root of your Drive:\n",
        "# # file_path = \"/content/drive/MyDrive/10academy_project/labeled_telegram_product_price_location.txt\" # <--- IMPORTANT: Adjust this path!\n",
        "# # print(f\"Looking for file at: {file_path}\")\n",
        "# # file_name = file_path # Use the full path as the file_name\n",
        "\n",
        "\n",
        "# Function to parse CoNLL formatted file\n",
        "def parse_conll_file(file_path):\n",
        "    \"\"\"Parses a CoNLL formatted file into a list of (words, tags) tuples.\"\"\"\n",
        "    texts = []\n",
        "    tags = []\n",
        "    current_words = []\n",
        "    current_tags = []\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line: # If line is not empty\n",
        "                parts = line.split()\n",
        "                if len(parts) == 2:\n",
        "                    word, tag = parts[0], parts[1]\n",
        "                    current_words.append(word)\n",
        "                    current_tags.append(tag)\n",
        "                else:\n",
        "                    # Handle malformed lines: if a line is not empty but doesn't have 2 parts, it's malformed\n",
        "                    print(f\"Warning: Skipping malformed line (expected 2 parts, got {len(parts)}): '{line}'\")\n",
        "            else: # Empty line indicates end of a sentence/message\n",
        "                if current_words: # Only add if there are words in the current sentence\n",
        "                    texts.append(current_words)\n",
        "                    tags.append(current_tags)\n",
        "                    current_words = []\n",
        "                    current_tags = []\n",
        "    # Add any remaining sentence at the end of the file (important if file doesn't end with blank line)\n",
        "    if current_words:\n",
        "        texts.append(current_words)\n",
        "        tags.append(current_tags)\n",
        "\n",
        "    return texts, tags\n",
        "\n",
        "# Parse your CoNLL file\n",
        "raw_texts, raw_tags = parse_conll_file(file_name)\n",
        "print(f\"Successfully parsed {len(raw_texts)} sentences from the CoNLL file.\")\n",
        "\n",
        "# Check for any tags in your data that are not in label_list\n",
        "all_unique_tags_in_data = set(tag for sublist in raw_tags for tag in sublist)\n",
        "missing_labels_in_config = all_unique_tags_in_data - set(label_list)\n",
        "if missing_labels_in_config:\n",
        "    print(f\"\\nWARNING: Found tags in your data not present in 'label_list': {missing_labels_in_config}\")\n",
        "    print(\"Please update 'label_list' in Cell 2 to include these tags and rerun all cells from the beginning.\")\n",
        "    # Consider adding `raise ValueError(\"Missing labels in config\")` here if you want to stop execution\n",
        "    # if this critical issue occurs.\n",
        "\n",
        "\n",
        "# Convert raw_tags (string labels) to numerical IDs\n",
        "numeric_tags = []\n",
        "for i, sentence_tags in enumerate(raw_tags):\n",
        "    current_numeric_tags = []\n",
        "    for tag in sentence_tags:\n",
        "        if tag in label2id:\n",
        "            current_numeric_tags.append(label2id[tag])\n",
        "        else:\n",
        "            # This case should ideally be caught by the warning above.\n",
        "            print(f\"Error: Tag '{tag}' not found in label2id for sentence {i}. Assigning 'O'.\")\n",
        "            current_numeric_tags.append(label2id[\"O\"])\n",
        "    numeric_tags.append(current_numeric_tags)\n",
        "\n",
        "\n",
        "# Create a Hugging Face Dataset\n",
        "features = Features({\n",
        "    'id': Value('string'),\n",
        "    'tokens': Sequence(Value('string')),\n",
        "    'ner_tags': Sequence(ClassLabel(names=label_list))\n",
        "})\n",
        "\n",
        "data_dict_list = []\n",
        "for i, (tokens, tags) in enumerate(zip(raw_texts, numeric_tags)):\n",
        "    # Ensure tokens and tags have the same length\n",
        "    if len(tokens) != len(tags):\n",
        "        print(f\"Warning: Token-tag length mismatch in sentence {i}. Skipping this sentence.\")\n",
        "        print(f\"Tokens: {tokens}\")\n",
        "        print(f\"Tags: {[id2label[t] for t in tags]}\") # Convert numerical tags back to string for printing\n",
        "        continue # Skip this malformed sentence\n",
        "    data_dict_list.append({\n",
        "        'id': str(i),\n",
        "        'tokens': tokens,\n",
        "        'ner_tags': tags\n",
        "    })\n",
        "\n",
        "dataset = Dataset.from_list(data_dict_list, features=features)\n",
        "print(f\"Dataset loaded with {len(dataset)} examples.\")\n",
        "print(\"Example from dataset (first entry):\")\n",
        "print(dataset[0])\n",
        "\n",
        "# Split into training and validation sets\n",
        "# Use a small validation set (e.g., 10-20% of your data). Adjust test_size as needed.\n",
        "train_test_split = dataset.train_test_split(test_size=0.2, seed=42) # Added seed for reproducibility\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
        "print(f\"Eval dataset size: {len(eval_dataset)}\")"
      ],
      "metadata": {
        "id": "CKNiGuGLrr2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n4. Tokenizing data and aligning labels...\")\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    # This function expects examples['tokens'] to be a list of lists of words (sentences)\n",
        "    # and examples['ner_tags'] to be a list of lists of numerical tag IDs.\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True, # Truncate long sequences to model's max input length\n",
        "        is_split_into_words=True # Tells the tokenizer that inputs are already pre-split into words\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]): # Iterate through each sentence's original labels\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i) # Get word IDs for the current tokenized sentence\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens (like CLS, SEP, PAD) have a word_idx of None.\n",
        "            # We set their label to -100 so they are ignored in loss computation.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # If this is the first token of a new word, assign its original label.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # If it's a subsequent subword token of the same word:\n",
        "            else:\n",
        "                # Get the original string label for the word\n",
        "                original_label_str = id2label[label[word_idx]]\n",
        "                # If the original label was a 'B-' tag, change it to 'I-'.\n",
        "                # Otherwise, keep it as 'I-' or 'O'. This ensures all subwords of an entity\n",
        "                # are labeled as 'I-' (or 'O' if the word was 'O').\n",
        "                if original_label_str.startswith(\"B-\"):\n",
        "                    label_ids.append(label2id[f\"I-{original_label_str[2:]}\"])\n",
        "                else:\n",
        "                    label_ids.append(label[word_idx]) # For I- and O tags, keep them as is\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the tokenization and alignment to both training and evaluation datasets\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "print(\"\\nExample of tokenized and aligned data (first entry from training set):\")\n",
        "first_example_tokenized = tokenized_train_dataset[0]\n",
        "print(\"Original Tokens (first sentence in training set):\", train_dataset[0][\"tokens\"])\n",
        "print(\"Original Labels:\", [id2label[l] for l in train_dataset[0][\"ner_tags\"]])\n",
        "print(\"Subword Tokens (after tokenization):\", tokenizer.convert_ids_to_tokens(first_example_tokenized[\"input_ids\"]))\n",
        "print(\"Aligned Numerical Labels:\", first_example_tokenized[\"labels\"])\n",
        "print(\"Decoded Aligned Labels:\", [id2label[l] if l != -100 else \"IGNORE\" for l in first_example_tokenized[\"labels\"]])"
      ],
      "metadata": {
        "id": "nqhtST3cr3dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n4. Tokenizing data and aligning labels...\")\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    # This function expects examples['tokens'] to be a list of lists of words (sentences)\n",
        "    # and examples['ner_tags'] to be a list of lists of numerical tag IDs.\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True, # Truncate long sequences to model's max input length\n",
        "        is_split_into_words=True # Tells the tokenizer that inputs are already pre-split into words\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]): # Iterate through each sentence's original labels\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i) # Get word IDs for the current tokenized sentence\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens (like CLS, SEP, PAD) have a word_idx of None.\n",
        "            # We set their label to -100 so they are ignored in loss computation.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # If this is the first token of a new word, assign its original label.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # If it's a subsequent subword token of the same word:\n",
        "            else:\n",
        "                # Get the original string label for the word\n",
        "                original_label_str = id2label[label[word_idx]]\n",
        "                # If the original label was a 'B-' tag, change it to 'I-'.\n",
        "                # Otherwise, keep it as 'I-' or 'O'. This ensures all subwords of an entity\n",
        "                # are labeled as 'I-' (or 'O' if the word was 'O').\n",
        "                if original_label_str.startswith(\"B-\"):\n",
        "                    label_ids.append(label2id[f\"I-{original_label_str[2:]}\"])\n",
        "                else:\n",
        "                    label_ids.append(label[word_idx]) # For I- and O tags, keep them as is\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the tokenization and alignment to both training and evaluation datasets\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "print(\"\\nExample of tokenized and aligned data (first entry from training set):\")\n",
        "first_example_tokenized = tokenized_train_dataset[0]\n",
        "print(\"Original Tokens (first sentence in training set):\", train_dataset[0][\"tokens\"])\n",
        "print(\"Original Labels:\", [id2label[l] for l in train_dataset[0][\"ner_tags\"]])\n",
        "print(\"Subword Tokens (after tokenization):\", tokenizer.convert_ids_to_tokens(first_example_tokenized[\"input_ids\"]))\n",
        "print(\"Aligned Numerical Labels:\", first_example_tokenized[\"labels\"])\n",
        "print(\"Decoded Aligned Labels:\", [id2label[l] if l != -100 else \"IGNORE\" for l in first_example_tokenized[\"labels\"]])"
      ],
      "metadata": {
        "id": "ZL4-fEOFtCcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up Training Arguments and Model\n",
        "print(\"\\n5. Setting up training arguments and model...\")\n",
        "\n",
        "# Initialize the Data Collator for Token Classification\n",
        "# This handles padding of sequences to the longest sequence in each batch,\n",
        "# and also stacks inputs into tensors.\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "# Load the model for token classification\n",
        "# This will add a classification head on top of the pre-trained model.\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=len(label_list), # Number of unique NER labels\n",
        "    id2label=id2label,         # Map numerical IDs back to string labels for output\n",
        "    label2id=label2id          # Map string labels to numerical IDs for internal use\n",
        ")\n",
        "\n",
        "# Verify if model's label configurations are correctly set\n",
        "print(f\"Model num_labels: {model.config.num_labels}\")\n",
        "print(f\"Model id2label: {model.config.id2label}\")\n",
        "print(f\"Model label2id: {model.config.label2id}\")\n",
        "\n",
        "# Define training arguments\n",
        "# These parameters significantly impact training time and model performance.\n",
        "# Adjust `num_train_epochs` and `per_device_train_batch_size` based on your dataset size\n",
        "# and available GPU memory.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",      # Evaluate at the end of each epoch (updated argument name)# Directory to save model checkpoints and training logs\n",
        "     learning_rate=2e-5,                       # Learning rate for the optimizer (typical for fine-tuning)\n",
        "    per_device_train_batch_size=16,           # Batch size per GPU/CPU during training\n",
        "    per_device_eval_batch_size=16,            # Batch size per GPU/CPU during evaluation\n",
        "    num_train_epochs=5,                       # Number of full passes over the training data\n",
        "    weight_decay=0.01,                        # L2 regularization to prevent overfitting\n",
        "    logging_dir=\"./logs\",                     # Directory for TensorBoard logs\n",
        "    logging_steps=100,                        # How often to log training information\n",
        "    save_strategy=\"epoch\",                    # Save a model checkpoint at the end of each epoch\n",
        "    save_total_limit=2,                       # Keep only the last 2 best checkpoints to save disk space\n",
        "    report_to=\"none\",                         # Disable integrations like Weights & Biases for simplicity\n",
        "    fp16=True,                                # Enable mixed precision training (float16) for faster GPU training\n",
        "    push_to_hub=False,                        # Do not push the model to the Hugging Face Hub automatically\n",
        "    load_best_model_at_end=True,              # Load the model with the best evaluation metric at the end of training\n",
        "    metric_for_best_model=\"overall_f1\",       # The metric to monitor for selecting the best model\n",
        "    greater_is_better=True,                   # For F1-score, a higher value is better\n",
        ")"
      ],
      "metadata": {
        "id": "N-2nMzNptJF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize and Run the Hugging Face Trainer\n",
        "print(\"\\n6. Initializing and running the Hugging Face Trainer...\")\n",
        "\n",
        "# Define the compute_metrics function for NER evaluation using seqeval\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    # Convert prediction logits to predicted label IDs\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Convert numerical labels and predictions back to string labels for seqeval\n",
        "    # Also, remove ignored index (-100)\n",
        "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # --- Important for seqeval ---\n",
        "    # Ensure lengths of prediction and true label lists are identical for each sample.\n",
        "    # This addresses potential issues where `word_ids` or `prediction` might cause slight mismatches.\n",
        "    cleaned_true_predictions = []\n",
        "    cleaned_true_labels = []\n",
        "    for pred_list, label_list_i in zip(true_predictions, true_labels):\n",
        "        if len(pred_list) == len(label_list_i):\n",
        "            cleaned_true_predictions.append(pred_list)\n",
        "            cleaned_true_labels.append(label_list_i)\n",
        "        else:\n",
        "            # This should ideally not happen if tokenization and alignment are robust.\n",
        "            # Print a warning if a mismatch occurs, indicating a potential data or alignment issue.\n",
        "            print(f\"Warning: Skipping a sample in metrics calculation due to length mismatch: pred={len(pred_list)}, label={len(label_list_i)}\")\n",
        "\n",
        "    if not cleaned_true_labels: # Handle case where all samples are skipped or no valid labels\n",
        "        return {\"overall_precision\": 0.0, \"overall_recall\": 0.0, \"overall_f1\": 0.0, \"overall_accuracy\": 0.0}\n",
        "\n",
        "    # Generate the classification report from seqeval\n",
        "    report = classification_report(cleaned_true_labels, cleaned_true_predictions, output_dict=True)\n",
        "\n",
        "    # Extract overall metrics, typically using 'micro avg' for overall performance in NER\n",
        "    overall_f1 = report['micro avg']['f1-score'] if 'micro avg' in report else f1_score(cleaned_true_labels, cleaned_true_predictions, average='micro')\n",
        "    overall_precision = report['micro avg']['precision'] if 'micro avg' in report else precision_score(cleaned_true_labels, cleaned_true_predictions, average='micro')\n",
        "    overall_recall = report['micro avg']['recall'] if 'micro avg' in report else recall_score(cleaned_true_labels, cleaned_true_predictions, average='micro')\n",
        "    overall_accuracy = accuracy_score(cleaned_true_labels, cleaned_true_predictions)\n",
        "\n",
        "    metrics = {\n",
        "        \"overall_precision\": overall_precision,\n",
        "        \"overall_recall\": overall_recall,\n",
        "        \"overall_f1\": overall_f1,\n",
        "        \"overall_accuracy\": overall_accuracy,\n",
        "    }\n",
        "\n",
        "    # Add per-entity F1 scores if they exist in the report (excluding 'O' tag and 'micro avg')\n",
        "    for entity_type in label_list:\n",
        "        # Check if the entity type is present in the report (i.e., it appeared in the eval set)\n",
        "        if entity_type != 'O' and entity_type in report:\n",
        "            metrics[f\"{entity_type}_f1\"] = report[entity_type]['f1-score']\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset, # The model will be evaluated on this dataset\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics, # Function to compute evaluation metrics\n",
        ")\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "# This will start the training loop. Progress bars and metrics will be displayed.\n",
        "trainer.train()\n",
        "print(\"\\nTraining complete!\")"
      ],
      "metadata": {
        "id": "XhOmkx4JuhMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the Fine-tuned Model (Final Evaluation)\n",
        "print(\"\\n7. Evaluating the fine-tuned model on the validation set (final check)...\")\n",
        "# This will run a final evaluation on the `eval_dataset` using the best model loaded at the end of training.\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Final Evaluation Results:\", eval_results)"
      ],
      "metadata": {
        "id": "eb1_uyRXu3fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n8. Saving the fine-tuned model and tokenizer...\")\n",
        "\n",
        "# Define a path to save your model\n",
        "# IMPORTANT: If you want to save to Google Drive for persistence, uncomment the Drive path\n",
        "# and ensure you have mounted Drive (Option B in Cell 3 was for this).\n",
        "model_save_path = \"./fine_tuned_amharic_ner_model_v1\" # Local path in Colab (temporary, deleted when session ends)\n",
        "# OR for Google Drive (recommended for persistence):\n",
        "# model_save_path = \"/content/drive/MyDrive/10academy_project/fine_tuned_amharic_ner_model_v1\" # <--- ADJUST THIS PATH!\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Save the model's weights, configuration, and vocabulary\n",
        "trainer.save_model(model_save_path) # Saves the model's weights and configuration\n",
        "tokenizer.save_pretrained(model_save_path) # Saves the tokenizer files (vocab, merges, etc.)\n",
        "\n",
        "print(f\"Model and tokenizer saved successfully to: {model_save_path}\")\n",
        "\n",
        "# Optional: If you saved to a local Colab path (not Drive), you might want to download it.\n",
        "# This zips the model directory and initiates a browser download.\n",
        "try:\n",
        "    if not \"MyDrive\" in model_save_path: # Only attempt download if not saved to Drive\n",
        "        print(\"\\nAttempting to zip and download the model (if saved locally)...\")\n",
        "        !zip -r /content/fine_tuned_amharic_ner_model_v1.zip {model_save_path}\n",
        "        from google.colab import files\n",
        "        files.download('/content/fine_tuned_amharic_ner_model_v1.zip')\n",
        "        print(\"Model zip file download initiated.\")\n",
        "    else:\n",
        "        print(\"Model saved to Google Drive, no need to download from Colab local storage.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not zip or download model (an error occurred or it was saved to Drive): {e}\")\n",
        "\n",
        "print(\"\\n--- Fine-tuning process complete ---\")"
      ],
      "metadata": {
        "id": "vhIjZwY2xB6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}